{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "c41a764b-c032-441c-aedc-3ec8bf1d2ae3"
   },
   "source": [
    "# Configure Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "edec7385-c0ce-4cbd-af22-0ce00a87a761"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "0b52c8cf-bb4a-4334-bd9c-eb90159cd5cc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'spark.app.id', u'app-20210708173228-0034')\n",
      "(u'spark.app.name', u'my-notebook')\n",
      "(u'spark.cores.max', u'2')\n",
      "(u'spark.driver.extraClassPath', u'/usr/local/spark/jars/commons-lang-2.6.jar:/dbdrivers/*')\n",
      "(u'spark.driver.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.driver.host', u'10.1.110.63')\n",
      "(u'spark.driver.port', u'38674')\n",
      "(u'spark.dynamicAllocation.enabled', u'true')\n",
      "(u'spark.dynamicAllocation.executorIdleTimeout', u'300')\n",
      "(u'spark.dynamicAllocation.initialExecutors', u'1')\n",
      "(u'spark.eventLog.dir', u'/tmp/spark-events')\n",
      "(u'spark.eventLog.enabled', u'true')\n",
      "(u'spark.executor.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.executor.id', u'driver')\n",
      "(u'spark.executor.memory', u'4g')\n",
      "(u'spark.master', u'spark://spark-master-svc:7077')\n",
      "(u'spark.port.maxRetries', u'100')\n",
      "(u'spark.rdd.compress', u'True')\n",
      "(u'spark.serializer.objectStreamReset', u'100')\n",
      "(u'spark.shuffle.service.enabled', u'true')\n",
      "(u'spark.sql.warehouse.dir', u'/tmp')\n",
      "(u'spark.submit.deployMode', u'client')\n",
      "(u'spark.ui.enabled', u'false')\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "484cc92c-676d-49d2-93fb-249d3b835e9d"
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "13efbcc4-eae3-41e3-8cfb-4a27494c2933"
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.cores.max\", \"6\")\n",
    "    .set(\"spark.dynamicAllocation.initialExecutors\", \"6\")\n",
    "    .set(\"spark.executor.cores\", \"1\")\n",
    "    .set(\"spark.executor.memory\", \"3g\"))\n",
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "ad7c60f3-ab66-4d1b-ae51-7d05f0e580c6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'spark.app.id', u'app-20210708182016-0038')\n",
      "(u'spark.app.name', u'pyspark-shell')\n",
      "(u'spark.cores.max', u'6')\n",
      "(u'spark.driver.extraClassPath', u'/dbdrivers/*')\n",
      "(u'spark.driver.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.driver.host', u'10.1.110.6')\n",
      "(u'spark.driver.port', u'40264')\n",
      "(u'spark.dynamicAllocation.enabled', u'true')\n",
      "(u'spark.dynamicAllocation.executorIdleTimeout', u'300')\n",
      "(u'spark.dynamicAllocation.initialExecutors', u'6')\n",
      "(u'spark.eventLog.dir', u'/tmp/spark-events')\n",
      "(u'spark.eventLog.enabled', u'true')\n",
      "(u'spark.executor.cores', u'1')\n",
      "(u'spark.executor.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.executor.id', u'driver')\n",
      "(u'spark.executor.memory', u'3g')\n",
      "(u'spark.master', u'spark://spark-master-svc:7077')\n",
      "(u'spark.port.maxRetries', u'100')\n",
      "(u'spark.rdd.compress', u'True')\n",
      "(u'spark.serializer.objectStreamReset', u'100')\n",
      "(u'spark.shuffle.service.enabled', u'true')\n",
      "(u'spark.sql.warehouse.dir', u'/tmp')\n",
      "(u'spark.submit.deployMode', u'client')\n",
      "(u'spark.ui.enabled', u'false')\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "a63bf697-3502-4a48-9c06-2a4608d01a63"
   },
   "source": [
    "## <font color='maroon'>This Notebook shall build and train a Spark model to predict customer churn using 1 million records as training data. </font>\n",
    "### <font color='navyblue'> The training data is available as a local file in WSL and also in a remote db2 database </font>   \n",
    "### <font color='navyblue'> The notebook shall load both sources and compare loading time </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "0c4f3fe3-4ec1-46d0-8e49-51cf327f1197"
   },
   "source": [
    "This notebook contains steps and code to develop a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.6 and Apache® Spark 2.4.\n",
    "\n",
    "You will use a data set, <B>Telco Customer Churn</B>, which details anonymous customer data from a telecommunication company. Use the details of this data set to predict customer churn which is very critical to business as it's easier to retain existing customers rather than acquiring new ones.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Load a CSV file into an Apache® Spark DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create an Apache® Spark machine learning pipeline.\n",
    "-  Train and evaluate a model.\n",
    "-  Persist a pipeline and model in Watson Machine Learning repository.\n",
    "-  Explore and visualize prediction results using the plotly package.\n",
    "-  Deploy a model for batch scoring using Wastson Machine Learning API.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Set up the environment](#setup)\n",
    "2.\t[Load and explore data](#load)\n",
    "3.\t[Create spark ml model](#model)\n",
    "4.\t[Persist model](#persistence)\n",
    "5.\t[Predict locally and visualize](#visualization)\n",
    "6.\t[Deploy and score](#scoring)\n",
    "7.  [Clean up](#cleanup)\n",
    "8.\t[Summary and next steps](#summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "1c51c963-bd56-43b7-91b0-88ecf59facdb"
   },
   "outputs": [],
   "source": [
    "# Step 1: Import Librraies\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "47c3e09f-d0d4-4a5a-b6cb-ed8c2db213ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 09/07/2021 01:37:41\n",
      "+------+---+----------+------+--------+--------+------------+-------+----------+---------+-----+--------------------+-----+---------+\n",
      "|GENDER|AGE|INVESTMENT|INCOME|ACTIVITY|YRLY_AMT|AVG_DAILY_TX|YRLY_TX|AVG_TX_AMT|NEGTWEETS|STATE|           EDUCATION|label|NUMOFROWS|\n",
      "+------+---+----------+------+--------+--------+------------+-------+----------+---------+-----+--------------------+-----+---------+\n",
      "|     F|  1|         0|     0|       1|       0|           3|      4|         0|        0|   AK|High school graduate|    0|      171|\n",
      "|     M|  1|         0|     0|       2|       0|           3|      4|         0|        0|   AK|High school graduate|    0|      171|\n",
      "|     F|  1|         0|     0|       5|       0|           3|      4|         0|        0|   AK|High school graduate|    0|      171|\n",
      "|     M|  1|         0|     0|       5|       0|           3|      4|         0|        0|   AK|High school graduate|    0|      171|\n",
      "|     M|  1|         0|     0|       1|       0|           3|      4|         0|        1|   AK|High school graduate|    0|      171|\n",
      "|     F|  1|         0|     0|       2|       0|           3|      4|         0|        1|   AK|    Associate degree|    0|      171|\n",
      "|     M|  1|         0|     0|       2|       0|           3|      4|         0|        1|   AK|High school graduate|    0|      171|\n",
      "|     F|  1|         0|     0|       5|       0|           3|      4|         0|        1|   AK|    Associate degree|    0|      171|\n",
      "|     F|  1|         0|     0|       2|       0|           3|      4|         0|        0|   AL|High school graduate|    0|      171|\n",
      "|     M|  1|         0|     0|       2|       0|           3|      4|         0|        0|   AL|High school graduate|    0|      171|\n",
      "|     M|  1|         0|     0|       4|       0|           3|      4|         0|        0|   AL|    Associate degree|    0|      342|\n",
      "|     F|  1|         0|     0|       1|       0|           3|      4|         0|        1|   AL|High school graduate|    0|      171|\n",
      "|     M|  1|         0|     0|       1|       0|           3|      4|         0|        1|   AL|    Associate degree|    0|      171|\n",
      "|     M|  1|         0|     0|       1|       0|           3|      4|         0|        1|   AL|High school graduate|    0|      171|\n",
      "|     F|  1|         0|     0|       2|       0|           3|      4|         0|        1|   AL|High school graduate|    0|      171|\n",
      "|     M|  1|         0|     0|       2|       0|           3|      4|         0|        1|   AL|High school graduate|    0|      513|\n",
      "|     F|  1|         0|     0|       3|       0|           3|      4|         0|        1|   AL|High school graduate|    0|      171|\n",
      "|     F|  1|         0|     0|       4|       0|           3|      4|         0|        1|   AL|High school graduate|    0|      342|\n",
      "|     F|  1|         0|     0|       5|       0|           3|      4|         0|        1|   AL|    Associate degree|    0|      171|\n",
      "|     M|  1|         0|     0|       5|       0|           3|      4|         0|        1|   AL|High school graduate|    0|      171|\n",
      "+------+---+----------+------+--------+--------+------------+-------+----------+---------+-----+--------------------+-----+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "End time: 09/07/2021 01:37:44\n",
      "Processing time 0:00:03.571706\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Db2 data\n",
    "\n",
    "\n",
    "import dsx_core_utils, requests, os, io\n",
    "\n",
    "ts2 = datetime.now()\n",
    "print(\"Start time:\", ts2.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "import dsx_core_utils, requests, os, io\n",
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession(sc).builder.getOrCreate()\n",
    "#\n",
    "# This sample code may not be suitable for large data sets\n",
    "#\n",
    "# Add asset from remote connection\n",
    "# df1 = None\n",
    "# dataSet = dsx_core_utils.get_remote_data_set_info('CUST_CHURN_1M')\n",
    "# dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n",
    "# sparkSession = SparkSession(sc).builder.getOrCreate()\n",
    "# # Load JDBC data to Spark dataframe\n",
    "# dbTableOrQuery = '\"' + (dataSet['schema'] + '\".\"' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table'] + '\"'\n",
    "# if (dataSet['query']):\n",
    "#     dbTableOrQuery = \"(\" + dataSet['query'] + \") TBL\"\n",
    "#df1 = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"query\", \"SELECT \"GENDER\" FROM HDW35292.CUST_CHURN_1M\").option(\"user\",dataSource['user']).option(\"password\",dataSource['password']).load()\n",
    "\n",
    "df = sparkSession.read.format(\"jdbc\").option(\"url\", 'jdbc:db2://dashdb-txn-sbox-yp-dal09-11.services.dal.bluemix.net:50000/BLUDB').option(\"query\", 'SELECT GENDER,CASE WHEN AGE < 20 THEN 0 WHEN AGE < 40 THEN 1 WHEN AGE < 60 THEN 2 WHEN AGE < 60 THEN 3 WHEN AGE < 80 THEN 4 ELSE 5 END  AGE, CASE WHEN INVESTMENT < 20000 THEN 0 WHEN INVESTMENT < 40000 THEN 1 WHEN INVESTMENT < 60000 THEN 2 WHEN INVESTMENT < 60000 THEN 3 WHEN INVESTMENT < 80000 THEN 4 ELSE 5 END  INVESTMENT, CASE WHEN INCOME < 1000000 THEN 0 WHEN INCOME < 2000000 THEN 1 WHEN INCOME < 3000000 THEN 2 WHEN INCOME < 4000000 THEN 3 WHEN INCOME < 5000000 THEN 4 ELSE 5 END  INCOME, ACTIVITY, CASE WHEN YRLY_AMT < 100000 THEN 0 WHEN YRLY_AMT < 200000 THEN 1 WHEN YRLY_AMT < 300000 THEN 2 WHEN YRLY_AMT < 400000 THEN 3 WHEN YRLY_AMT < 500000 THEN 4 ELSE 5 END  YRLY_AMT, CASE WHEN AVG_DAILY_TX < .2 THEN 0 WHEN AVG_DAILY_TX < .4 THEN 1 WHEN AVG_DAILY_TX < .6 THEN 2 WHEN AVG_DAILY_TX < .8 THEN 3 WHEN AVG_DAILY_TX < 1 THEN 4 ELSE 5 END  AVG_DAILY_TX, CASE WHEN YRLY_TX < 50   THEN 0 WHEN YRLY_TX < 100  THEN 1 WHEN YRLY_TX < 150  THEN 2 WHEN YRLY_TX < 200 THEN 3 WHEN YRLY_TX < 250 THEN 4 ELSE 5 END  YRLY_TX, CASE WHEN AVG_TX_AMT < 500  THEN 0 WHEN AVG_TX_AMT < 1000  THEN 1 WHEN AVG_TX_AMT < 1500  THEN 2 WHEN AVG_TX_AMT < 2000 THEN 3 WHEN AVG_TX_AMT < 2500 THEN 4 ELSE 5 END  AVG_TX_AMT, CASE WHEN NEGTWEETS < 3  THEN 0 WHEN NEGTWEETS < 6  THEN 1 WHEN NEGTWEETS < 9  THEN 2 WHEN NEGTWEETS < 12 THEN 3 WHEN NEGTWEETS < 15 THEN 4 ELSE 5 END  NEGTWEETS,   STATE, EDUCATION, \"label\", count(1)  NUMOFROWS FROM   HDW35292.CUST_CHURN_1M group by GENDER, CASE  WHEN AGE < 20 THEN 0 WHEN AGE < 40 THEN 1  WHEN AGE < 60 THEN 2 WHEN AGE < 60 THEN 3 WHEN AGE < 80 THEN 4 ELSE 5 END, CASE  WHEN INVESTMENT < 20000 THEN 0 WHEN INVESTMENT < 40000 THEN 1 WHEN INVESTMENT < 60000 THEN 2 WHEN INVESTMENT < 60000 THEN 3 WHEN INVESTMENT < 80000 THEN 4 ELSE 5 END, CASE WHEN INCOME < 1000000 THEN 0 WHEN INCOME < 2000000 THEN 1 WHEN INCOME < 3000000 THEN 2 WHEN INCOME < 4000000 THEN 3 WHEN INCOME < 5000000 THEN 4 ELSE 5 END, ACTIVITY, CASE WHEN YRLY_AMT < 100000 THEN 0 WHEN YRLY_AMT < 200000 THEN 1 WHEN YRLY_AMT < 300000 THEN 2 WHEN YRLY_AMT < 400000 THEN 3 WHEN YRLY_AMT < 500000 THEN 4 ELSE 5 END , CASE WHEN AVG_DAILY_TX < .2 THEN 0 WHEN AVG_DAILY_TX < .4 THEN 1 WHEN AVG_DAILY_TX < .6 THEN 2 WHEN AVG_DAILY_TX < .8 THEN 3 WHEN AVG_DAILY_TX < 1 THEN 4 ELSE 5 END, CASE WHEN YRLY_TX < 50   THEN 0 WHEN YRLY_TX < 100  THEN 1 WHEN YRLY_TX < 150  THEN 2 WHEN YRLY_TX < 200 THEN 3 WHEN YRLY_TX < 250 THEN 4 ELSE 5 END, CASE WHEN AVG_TX_AMT < 500  THEN 0 WHEN AVG_TX_AMT < 1000  THEN 1 WHEN AVG_TX_AMT < 1500  THEN 2 WHEN AVG_TX_AMT < 2000 THEN 3 WHEN AVG_TX_AMT < 2500 THEN 4 ELSE 5 END, CASE WHEN NEGTWEETS < 3  THEN 0 WHEN NEGTWEETS < 6  THEN 1 WHEN NEGTWEETS < 9  THEN 2 WHEN NEGTWEETS < 12 THEN 3 WHEN NEGTWEETS < 15 THEN 4 ELSE 5 END  ,STATE,EDUCATION,\"label\"').option(\"user\",'hdw35292').option(\"password\",'twpkq8^2zswkwvg4').load()\n",
    "df.show()\n",
    "\n",
    "te2 = datetime.now()\n",
    "print(\"End time:\", te2.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"Processing time\", te2-ts2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "0cf07a05-1c98-42b9-8626-4ea2932a2c6e"
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"NUMOFROWS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "f31ade12-f607-4e9a-9fbc-7b5699cf1022"
   },
   "source": [
    "Explore the loaded data by using the following Apache® Spark DataFrame methods:\n",
    "-  print schema\n",
    "-  count all records\n",
    "-  show distribution of label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "7ae95caf-6312-436d-88a4-41411c7173cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time: 09/07/2021 01:37:45\n",
      "root\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- INVESTMENT: integer (nullable = true)\n",
      " |-- INCOME: integer (nullable = true)\n",
      " |-- ACTIVITY: integer (nullable = true)\n",
      " |-- YRLY_AMT: integer (nullable = true)\n",
      " |-- AVG_DAILY_TX: integer (nullable = true)\n",
      " |-- YRLY_TX: integer (nullable = true)\n",
      " |-- AVG_TX_AMT: integer (nullable = true)\n",
      " |-- NEGTWEETS: integer (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ts2 = datetime.now()\n",
    "print(\"Start time:\", ts2.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "593fba16-9b9f-4465-9074-b8a0a02310b0"
   },
   "source": [
    "As you can see, the data contains 21 fields. \"Churn\" field is the one we would like to predict (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "53e6ad24-1283-40f4-b1a1-9a1834c9bbac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 4870\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of records: \" + str(df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "978549dd-c26e-4132-8322-91e610062e9f"
   },
   "source": [
    "Now you will check if all records have complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "b07a9e7d-8bed-4506-883e-2369ee33051f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with complete data: 4870\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "print(\"Number of records with complete data: %3g\" % df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "c02630a8-628f-4261-b97f-f7c3c6b9eb21"
   },
   "source": [
    "You can see that there are some missing values you can investigate that all missing values are present in TotalCharges feature. We will use dataset with missing values removed for model training and evaluation.\n",
    "Now you will inspect distribution of classes in label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "10bc89fb-9756-4699-a7ca-fc633961a219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 1192|\n",
      "|    0| 3678|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "35007e89-56c7-4c9d-a4ad-9a045b8e7918"
   },
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create an Apache® Spark machine learning model\n",
    "\n",
    "In this section you will learn how to prepare data, create an Apache® Spark machine learning pipeline, and train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "0ff8444f-f656-47ca-89de-09c445ace100"
   },
   "source": [
    "### 3.1: Prepare data\n",
    "\n",
    "In this subsection you will split your data into: train, test and predict datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "73721fc8-47cb-485c-ac6a-07f3d2434bb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for training: 3911\n",
      "Number of records for evaluation: 857\n",
      "Number of records for prediction: 102\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data, predict_data) = df.randomSplit([0.8, 0.18, 0.02], 24)\n",
    "\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))\n",
    "print(\"Number of records for prediction: \" + str(predict_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "be65b2db-b57a-4a1d-8941-fd197dd42c2c"
   },
   "source": [
    "As you can see our data has been successfully split into three datasets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "32f2ff99-8da9-4744-a00f-b804b9fbdee0"
   },
   "source": [
    "### 3.2: Create pipeline and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "920e77cf-ade2-4bd1-bd3d-44eef9bbd3da"
   },
   "source": [
    "In this section you will create an Apache® Spark machine learning pipeline and then train the model.\n",
    "In the first step you need to import the Apache® Spark machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "60d10a34-5f6d-4c16-a813-3a495104a6bd"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString, RFormula\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "7bbc72a4-9ff4-47b0-83e1-54ff30fb89e8"
   },
   "outputs": [],
   "source": [
    "lab = StringIndexer(inputCol = 'label', outputCol = 'labelpredict')\n",
    "features = RFormula(formula = \"~ GENDER + AGE +  INVESTMENT + INCOME + ACTIVITY + YRLY_AMT + AVG_DAILY_TX + YRLY_TX + AVG_TX_AMT + NEGTWEETS + STATE + EDUCATION - 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "9c75d782-a735-4433-86da-3e3420676da4"
   },
   "source": [
    "Next, define estimators you want to use for classification. Logistic Regression is used in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "47cec92e-28c9-4218-9614-d77f2ca090fa"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "bb935317-b369-4e01-afb9-00eceda961a8"
   },
   "source": [
    "Let's build the pipeline now. A pipeline consists of transformers and an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "813f7214-7340-46b3-a644-27c78dede035"
   },
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages = [features, lab , lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "c5108fc1-e0aa-48f2-bd11-f3940b85c0a4"
   },
   "source": [
    "Now, you can train your Logistic Regression model using the previously defined **pipeline** and **train data**.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "1e371e23-1676-4c8a-b398-07fb6b77515b"
   },
   "outputs": [],
   "source": [
    "model_lr = pipeline_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "112daf09-577f-4e56-97fc-f343b01ba806"
   },
   "source": [
    "You can check your **model accuracy** now. To evaluate the model, use **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "f5809895-3414-4ebc-be4e-6d2473410bbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset:\n",
      "Accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "predictions = model_lr.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelpredict\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "print(\"Accuracy = %3.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "id": "dc55bf78-0833-45b4-8014-97fd450df092"
   },
   "source": [
    "You can tune your model now to achieve better accuracy. For simplicity of this example tuning section is omitted.\n",
    "\n",
    "<a id=\"persistence\"></a>\n",
    "## 4. Persist model\n",
    "\n",
    "\n",
    "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository using Python client libraries.\n",
    "\n",
    "**Note**: Apache® Spark 2.4 is required.\n",
    "    \n",
    "### 4.1: Save pipeline and model\n",
    "\n",
    "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "53c98ac2-d833-48f8-8a97-4593e795725a"
   },
   "outputs": [],
   "source": [
    "from dsx_ml.ml import save\n",
    "\n",
    "save(name = 'ChurnPredictdb2prep',\n",
    "     model = model_lr,\n",
    "     test_data = test_data,\n",
    "     algorithm_type = 'Classification',\n",
    "     source='Cust_churn_1m_db_preprocess.ipynb',\n",
    "     description='This is a sample description for a spark model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "8c4e949f-21f0-4e72-af3b-03483de90d8b"
   },
   "outputs": [],
   "source": [
    "te2 = datetime.now()\n",
    "print(\"End time:\", te2.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"Processing time\", te2-ts2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
