{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Configure Spark Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'spark.app.id', u'app-20210708173228-0034')\n",
      "(u'spark.app.name', u'my-notebook')\n",
      "(u'spark.cores.max', u'2')\n",
      "(u'spark.driver.extraClassPath', u'/usr/local/spark/jars/commons-lang-2.6.jar:/dbdrivers/*')\n",
      "(u'spark.driver.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.driver.host', u'10.1.110.63')\n",
      "(u'spark.driver.port', u'38674')\n",
      "(u'spark.dynamicAllocation.enabled', u'true')\n",
      "(u'spark.dynamicAllocation.executorIdleTimeout', u'300')\n",
      "(u'spark.dynamicAllocation.initialExecutors', u'1')\n",
      "(u'spark.eventLog.dir', u'/tmp/spark-events')\n",
      "(u'spark.eventLog.enabled', u'true')\n",
      "(u'spark.executor.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.executor.id', u'driver')\n",
      "(u'spark.executor.memory', u'4g')\n",
      "(u'spark.master', u'spark://spark-master-svc:7077')\n",
      "(u'spark.port.maxRetries', u'100')\n",
      "(u'spark.rdd.compress', u'True')\n",
      "(u'spark.serializer.objectStreamReset', u'100')\n",
      "(u'spark.shuffle.service.enabled', u'true')\n",
      "(u'spark.sql.warehouse.dir', u'/tmp')\n",
      "(u'spark.submit.deployMode', u'client')\n",
      "(u'spark.ui.enabled', u'false')\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.cores.max\", \"6\")\n",
    "    .set(\"spark.dynamicAllocation.initialExecutors\", \"6\")\n",
    "    .set(\"spark.executor.cores\", \"1\")\n",
    "    .set(\"spark.executor.memory\", \"3g\"))\n",
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'spark.app.id', u'app-20210708182016-0038')\n",
      "(u'spark.app.name', u'pyspark-shell')\n",
      "(u'spark.cores.max', u'6')\n",
      "(u'spark.driver.extraClassPath', u'/dbdrivers/*')\n",
      "(u'spark.driver.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.driver.host', u'10.1.110.6')\n",
      "(u'spark.driver.port', u'40264')\n",
      "(u'spark.dynamicAllocation.enabled', u'true')\n",
      "(u'spark.dynamicAllocation.executorIdleTimeout', u'300')\n",
      "(u'spark.dynamicAllocation.initialExecutors', u'6')\n",
      "(u'spark.eventLog.dir', u'/tmp/spark-events')\n",
      "(u'spark.eventLog.enabled', u'true')\n",
      "(u'spark.executor.cores', u'1')\n",
      "(u'spark.executor.extraJavaOptions', u'-Djavax.net.ssl.trustStore=/user-home/_global_/security/customer-truststores/cacerts')\n",
      "(u'spark.executor.id', u'driver')\n",
      "(u'spark.executor.memory', u'3g')\n",
      "(u'spark.master', u'spark://spark-master-svc:7077')\n",
      "(u'spark.port.maxRetries', u'100')\n",
      "(u'spark.rdd.compress', u'True')\n",
      "(u'spark.serializer.objectStreamReset', u'100')\n",
      "(u'spark.shuffle.service.enabled', u'true')\n",
      "(u'spark.sql.warehouse.dir', u'/tmp')\n",
      "(u'spark.submit.deployMode', u'client')\n",
      "(u'spark.ui.enabled', u'false')\n"
     ]
    }
   ],
   "source": [
    "for item in sorted(sc._conf.getAll()): print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## <font color='maroon'>This Notebook shall build and train a Spark model to predict customer churn using 1 million records as training data. </font>\n",
    "### <font color='navyblue'> The training data is available as a local file in WSL and also in a remote db2 database </font>   \n",
    "### <font color='navyblue'> The notebook shall load both sources and compare loading time </font>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook contains steps and code to develop a predictive model, and start scoring new data. This notebook introduces commands for getting data and for basic data cleaning and exploration, pipeline creation, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.6 and Apache® Spark 2.4.\n",
    "\n",
    "You will use a data set, <B>Telco Customer Churn</B>, which details anonymous customer data from a telecommunication company. Use the details of this data set to predict customer churn which is very critical to business as it's easier to retain existing customers rather than acquiring new ones.\n",
    "\n",
    "## Learning goals\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "-  Load a CSV file into an Apache® Spark DataFrame.\n",
    "-  Explore data.\n",
    "-  Prepare data for training and evaluation.\n",
    "-  Create an Apache® Spark machine learning pipeline.\n",
    "-  Train and evaluate a model.\n",
    "-  Persist a pipeline and model in Watson Machine Learning repository.\n",
    "-  Explore and visualize prediction results using the plotly package.\n",
    "-  Deploy a model for batch scoring using Wastson Machine Learning API.\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "\n",
    "1.\t[Set up the environment](#setup)\n",
    "2.\t[Load and explore data](#load)\n",
    "3.\t[Create spark ml model](#model)\n",
    "4.\t[Persist model](#persistence)\n",
    "5.\t[Predict locally and visualize](#visualization)\n",
    "6.\t[Deploy and score](#scoring)\n",
    "7.  [Clean up](#cleanup)\n",
    "8.\t[Summary and next steps](#summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start time:', '09/07/2021 01:41:03')\n",
      "+-----------+-----------+------+---+----------+-------+--------+--------+------------+-------+----------+---------+-----+----------------+-----+\n",
      "|CONTRACT_NR|CUSTOMER_NR|GENDER|AGE|INVESTMENT| INCOME|ACTIVITY|YRLY_AMT|AVG_DAILY_TX|YRLY_TX|AVG_TX_AMT|NEGTWEETS|STATE|       EDUCATION|label|\n",
      "+-----------+-----------+------+---+----------+-------+--------+--------+------------+-------+----------+---------+-----+----------------+-----+\n",
      "|   93011551|    2141912|     F| 84|    114368|3852862|       5|700259.0|    0.917808|    335|   2090.32|        3|   TX|Bachelors degree|    0|\n",
      "|   99352651|    4970498|     F| 44|     90298|3849843|       1|726977.0|    0.950685|    347|   2095.04|        2|   CA|Bachelors degree|    0|\n",
      "|   97002068|     755732|     F| 23|     94881|3217364|       1|579084.0|    0.920548|    336|   1723.46|        5|   CA|Bachelors degree|    1|\n",
      "|   96734455|    2887915|     F| 24|    112099|2438218|       4|470964.0|    0.994521|    363| 1297.4199|        2|   WA|Bachelors degree|    1|\n",
      "|   98381258|    3862465|     F| 67|     84638|2428245|       3|446615.0|    0.917808|    335| 1333.1799|        3|   CT|       Doctorate|    0|\n",
      "+-----------+-----------+------+---+----------+-------+--------+--------+------------+-------+----------+---------+-----+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "('End time:', '09/07/2021 01:41:10')\n",
      "('Processing time', datetime.timedelta(0, 7, 414902))\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load local file\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "ts1 = datetime.now()\n",
    "print(\"Start time:\", ts1.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "#\n",
    "# This sample code may not be suitable for large data sets\n",
    "#\n",
    "# Add asset from file system\n",
    "churn1mlocalcsv = SQLContext(sc).read.csv(os.environ['DSX_PROJECT_DIR']+'/datasets/cust_churn_1m.csv', header='true', inferSchema = 'true')\n",
    "churn1mlocalcsv.show(5)\n",
    "\n",
    "te1 = datetime.now()\n",
    "print(\"End time:\", te1.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"Processing time\", te1-ts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start time:', '09/07/2021 01:41:10')\n",
      "+-----------+-----------+------+---+----------+-------+--------+-----------+------------+-------+-----------+---------+-----+----------------+-----+\n",
      "|CONTRACT_NR|CUSTOMER_NR|GENDER|AGE|INVESTMENT| INCOME|ACTIVITY|   YRLY_AMT|AVG_DAILY_TX|YRLY_TX| AVG_TX_AMT|NEGTWEETS|STATE|       EDUCATION|label|\n",
      "+-----------+-----------+------+---+----------+-------+--------+-----------+------------+-------+-----------+---------+-----+----------------+-----+\n",
      "|   93011551|    2141912|     F| 84|    114368|3852862|       5|700259.0000|    0.917808|    335|2090.320000|        3|   TX|Bachelors degree|    0|\n",
      "|   99352651|    4970498|     F| 44|     90298|3849843|       1|726977.0000|    0.950685|    347|2095.040000|        2|   CA|Bachelors degree|    0|\n",
      "|   97002068|     755732|     F| 23|     94881|3217364|       1|579084.0000|    0.920548|    336|1723.460000|        5|   CA|Bachelors degree|    1|\n",
      "|   96734455|    2887915|     F| 24|    112099|2438218|       4|470964.0000|    0.994521|    363|1297.419900|        2|   WA|Bachelors degree|    1|\n",
      "|   98381258|    3862465|     F| 67|     84638|2428245|       3|446615.0000|    0.917808|    335|1333.179900|        3|   CT|       Doctorate|    0|\n",
      "+-----------+-----------+------+---+----------+-------+--------+-----------+------------+-------+-----------+---------+-----+----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "('End time:', '09/07/2021 01:41:15')\n",
      "('Processing time', datetime.timedelta(0, 4, 531963))\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load Db2 data\n",
    "\n",
    "import dsx_core_utils, requests, os, io\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ts2 = datetime.now()\n",
    "print(\"Start time:\", ts2.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "\n",
    "#\n",
    "# This sample code may not be suitable for large data sets\n",
    "#\n",
    "# Add asset from remote connection\n",
    "df1 = None\n",
    "dataSet = dsx_core_utils.get_remote_data_set_info('CUST_CHURN_1M')\n",
    "dataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n",
    "sparkSession = SparkSession(sc).builder.getOrCreate()\n",
    "# Load JDBC data to Spark dataframe\n",
    "dbTableOrQuery = '\"' + (dataSet['schema'] + '\".\"' if(len(dataSet['schema'].strip()) != 0) else '') + dataSet['table'] + '\"'\n",
    "if (dataSet['query']):\n",
    "    dbTableOrQuery = \"(\" + dataSet['query'] + \") TBL\"\n",
    "churnspark1mdb2 = sparkSession.read.format(\"jdbc\").option(\"url\", dataSource['URL']).option(\"dbtable\", dbTableOrQuery).option(\"user\",dataSource['user']).option(\"password\",dataSource['password']).load()\n",
    "churnspark1mdb2.show(5)\n",
    "\n",
    "te2 = datetime.now()\n",
    "print(\"End time:\", te2.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"Processing time\", te2-ts2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Explore the loaded data by using the following Apache® Spark DataFrame methods:\n",
    "-  print schema\n",
    "-  count all records\n",
    "-  show distribution of label classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CONTRACT_NR: integer (nullable = true)\n",
      " |-- CUSTOMER_NR: integer (nullable = true)\n",
      " |-- GENDER: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- INVESTMENT: integer (nullable = true)\n",
      " |-- INCOME: integer (nullable = true)\n",
      " |-- ACTIVITY: integer (nullable = true)\n",
      " |-- YRLY_AMT: double (nullable = true)\n",
      " |-- AVG_DAILY_TX: double (nullable = true)\n",
      " |-- YRLY_TX: integer (nullable = true)\n",
      " |-- AVG_TX_AMT: double (nullable = true)\n",
      " |-- NEGTWEETS: integer (nullable = true)\n",
      " |-- STATE: string (nullable = true)\n",
      " |-- EDUCATION: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn1mlocalcsv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see, the data contains 21 fields. \"Churn\" field is the one we would like to predict (label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records: 1025740\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of records: \" + str(churn1mlocalcsv.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now you will check if all records have complete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records with complete data: 1.02574e+06\n"
     ]
    }
   ],
   "source": [
    "churn1mlocal_df = churn1mlocalcsv.dropna()\n",
    "\n",
    "print(\"Number of records with complete data: %3g\" % churn1mlocal_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can see that there are some missing values you can investigate that all missing values are present in TotalCharges feature. We will use dataset with missing values removed for model training and evaluation.\n",
    "Now you will inspect distribution of classes in label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|261351|\n",
      "|    0|764389|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "churn1mlocal_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Create an Apache® Spark machine learning model\n",
    "\n",
    "In this section you will learn how to prepare data, create an Apache® Spark machine learning pipeline, and train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.1: Prepare data\n",
    "\n",
    "In this subsection you will split your data into: train, test and predict datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for training: 820724\n",
      "Number of records for evaluation: 184454\n",
      "Number of records for prediction: 20562\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data, predict_data) = churn1mlocal_df.randomSplit([0.8, 0.18, 0.02], 24)\n",
    "\n",
    "print(\"Number of records for training: \" + str(train_data.count()))\n",
    "print(\"Number of records for evaluation: \" + str(test_data.count()))\n",
    "print(\"Number of records for prediction: \" + str(predict_data.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As you can see our data has been successfully split into three datasets: \n",
    "\n",
    "-  The train data set, which is the largest group, is used for training.\n",
    "-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n",
    "-  The predict data set will be used for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3.2: Create pipeline and train a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this section you will create an Apache® Spark machine learning pipeline and then train the model.\n",
    "In the first step you need to import the Apache® Spark machine learning packages that will be needed in the subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, IndexToString, RFormula\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lab = StringIndexer(inputCol = 'label', outputCol = 'labelpredict')\n",
    "features = RFormula(formula = \"~ GENDER + AGE +  INVESTMENT + INCOME + ACTIVITY + YRLY_AMT + AVG_DAILY_TX + YRLY_TX + AVG_TX_AMT + NEGTWEETS + STATE + EDUCATION - 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, define estimators you want to use for classification. Logistic Regression is used in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's build the pipeline now. A pipeline consists of transformers and an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pipeline_lr = Pipeline(stages = [features, lab , lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, you can train your Logistic Regression model using the previously defined **pipeline** and **train data**.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_lr = pipeline_lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can check your **model accuracy** now. To evaluate the model, use **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset:\n",
      "Accuracy = 0.98\n"
     ]
    }
   ],
   "source": [
    "predictions = model_lr.transform(test_data)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelpredict\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Test dataset:\")\n",
    "print(\"Accuracy = %3.2f\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "You can tune your model now to achieve better accuracy. For simplicity of this example tuning section is omitted.\n",
    "\n",
    "<a id=\"persistence\"></a>\n",
    "## 4. Persist model\n",
    "\n",
    "\n",
    "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository using Python client libraries.\n",
    "\n",
    "**Note**: Apache® Spark 2.4 is required.\n",
    "    \n",
    "### 4.1: Save pipeline and model\n",
    "\n",
    "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/user-home/999/DSX_Projects/CustomerChurnLab/models/ChurnPredict/4',\n",
       " 'scoring_endpoint': 'https://dsxl-api/v3/project/score/Python27/spark-2.0/CustomerChurnLab/ChurnPredict/4'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dsx_ml.ml import save\n",
    "\n",
    "save(name = 'ChurnPredict',\n",
    "     model = model_lr,\n",
    "     test_data = test_data,\n",
    "     algorithm_type = 'Classification',\n",
    "     source='Cust_churn_1m.ipynb',\n",
    "     description='This is a sample description for a spark model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('End time:', '09/07/2021 01:42:35')\n",
      "('Processing time', datetime.timedelta(0, 84, 795467))\n"
     ]
    }
   ],
   "source": [
    "te2 = datetime.now()\n",
    "print(\"End time:\", te2.strftime(\"%d/%m/%Y %H:%M:%S\"))\n",
    "print(\"Processing time\", te2-ts2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python2.7 with Watson Studio Spark 2.0.2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
